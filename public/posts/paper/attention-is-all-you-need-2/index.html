<!doctype html>













































<html
  class="not-ready lg:text-base"
  style="--bg: #faf8f1"
  lang="ko-KR"
  dir="ltr"
>
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>[논문리뷰] Attention is all you need #2 - 누누타운</title>

  
  <meta name="theme-color" />

  
  
  
  
  <meta name="description" content="Attention, Self-attention
일반적으로 어텐션이란, 입력값에서 중요한 단어들에 더 집중할 수 있도록 한다.
문장을 병렬적으로 처리하기 때문에 속도가 비교적 빠르다.
RNN구조에서는 순차적으로 값을 확인하기 때문에 gradient vanishing/exploding문제가 발생하게 된다.
Self-attention
같은 문장 내에서 단어들 간의 관계를 나타낸다. attention의 입력값은 Q,K,V
Q,K,V
어텐션의 목표는 value의 weighted sum을 구하는것이고, 각 가중치는 Q,K가 얼마나 유사한지에 따라 결정된다.
Query - 소스벡터, 유사도를 계산하는 값
Key - 타겟벡터, 유사도를 계산하는 값
Value - key에 해당하는 정보로 값을 계산, 최종 출력계산에 사용
각각의 값은 인풋이 Linear연산을 거쳐서 구한다. - 벡터의 차원을 줄여준다.
Q.size = n x d_K
K.size = m x d_K
V. size = m x d_V
Z.size = n x d_V
왜 실습에서는 3차원이지? -&gt; 각 단어의 갯수만큼 연산하므로 n_Q, n_K, n_V가 존재
Q, K 의 dimension은 동일해야함
n_batch가 의미하는 것????? - data의 개수, ‘문장’의 개수
dim_V의 값은 정해져있지 않다. 어차피 Linear연산을 마지막에 한번 더 수행하여 z를 얻는다." />
  <meta name="author" content="Nunu" />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="http://localhost:1313/main.min.css" />

  
  
  
  
  
  <link rel="preload" as="image" href="http://localhost:1313/theme.png" />

  
  
  
  
  <link rel="preload" as="image" href="https://www.gravatar.com/avatar/b36b25ee4e567b6817a807d01980a628?s=160&amp;d=identicon" />
  
  

  
  
  <link rel="preload" as="image" href="http://localhost:1313/github.svg" />
  
  <link rel="preload" as="image" href="http://localhost:1313/rss.svg" />
  
  

  
  
  <script
    defer
    src="http://localhost:1313/highlight.min.js"
    onload="hljs.initHighlightingOnLoad();"
  ></script>
  

  
  
  
  
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css"
  integrity="sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI"
  crossorigin="anonymous"
/>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js"
  integrity="sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t"
  crossorigin="anonymous"
></script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js"
  integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
  crossorigin="anonymous"
></script>


<script>
  document.addEventListener('DOMContentLoaded', () =>
    renderMathInElement(document.body, {
      
      
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '$', right: '$', display: false },
      ],
      
      throwOnError: false,
    }),
  );
</script>

  
  
  

  
  <link
    rel="icon"
    href="http://localhost:1313/favicon.ico"
  />
  <link
    rel="apple-touch-icon"
    href="http://localhost:1313/apple-touch-icon.png"
  />

  
  <meta name="generator" content="Hugo 0.140.0">

  
  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[4.5rem] max-w-[--w] px-8 lg:justify-center">
  <div class="relative z-50 ltr:mr-auto rtl:ml-auto flex items-center">
    <a class="-translate-y-[1px] text-2xl font-medium" href="http://localhost:1313/"
      >누누타운</a
    >
    <div
      class="btn-dark text-[0] ltr:ml-4 rtl:mr-4 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.png)_left_center/_auto_theme('spacing.6')_no-repeat] [transition:_background-position_0.4s_steps(5)] dark:[background-position:right]"
      role="button"
      aria-label="Dark"
    ></div>
  </div>

  <div
    class="btn-menu relative z-50 ltr:-mr-8 rtl:-ml-8 flex h-[4.5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
    role="button"
    aria-label="Menu"
  ></div>

  

  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = '#faf8f1'.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-10 rtl:space-x-reverse">
      
      <a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >About</a
      >
      
      <a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/categories/"
        >categories</a
      >
      
      <a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/series/"
        >series</a
      >
      
      <a
        class="block text-center text-xl leading-[5rem] lg:text-base lg:font-normal"
        href="/archives/"
        >archives</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 rtl:space-x-reverse dark:invert ltr:lg:ml-14 rtl:lg:mr-14 lg:mt-0 lg:items-center"
    >
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href="https://github.com/dnjdsxor21"
        target="_blank"
        rel="me"
      >
        github
      </a>
      
      <a
        class="h-7 w-7 text-[0] [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href="http://localhost:1313/index.xml"
        target="_blank"
        rel="alternate"
      >
        rss
      </a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100vh-9rem)] max-w-[--w] px-8 pb-16 pt-14 dark:prose-invert"
    >
      

<article>
  <header class="mb-14">
    <h1 class="!my-0 pb-2.5">[논문리뷰] Attention is all you need #2</h1>

    
    <div class="text-xs antialiased opacity-60">
      
      <time>2023. 4. 3.</time>
      
      
      
      
    </div>
    
  </header>

  <section><h2 id="attention-self-attention">Attention, Self-attention</h2>
<p>일반적으로 어텐션이란, 입력값에서 중요한 단어들에 더 집중할 수 있도록 한다.<br>
문장을 병렬적으로 처리하기 때문에 속도가 비교적 빠르다.<br>
RNN구조에서는 순차적으로 값을 확인하기 때문에 gradient vanishing/exploding문제가 발생하게 된다.</p>
<h2 id="self-attention">Self-attention</h2>
<p>같은 문장 내에서 단어들 간의 관계를 나타낸다. attention의 입력값은 Q,K,V</p>
<h2 id="qkv">Q,K,V</h2>
<p>어텐션의 목표는 value의 weighted sum을 구하는것이고, 각 가중치는 Q,K가 얼마나 유사한지에 따라 결정된다.<br>
Query - 소스벡터, 유사도를 계산하는 값<br>
Key - 타겟벡터, 유사도를 계산하는 값<br>
Value - key에 해당하는 정보로 값을 계산, 최종 출력계산에 사용<br>
각각의 값은 인풋이 Linear연산을 거쳐서 구한다. - 벡터의 차원을 줄여준다.<br>
Q.size = n x d_K<br>
K.size = m x d_K<br>
V. size = m x d_V<br>
Z.size = n x d_V<br>
왜 실습에서는 3차원이지? -&gt; 각 단어의 갯수만큼 연산하므로 n_Q, n_K, n_V가 존재<br>
Q, K 의 dimension은 동일해야함<br>
n_batch가 의미하는 것????? - data의 개수, ‘문장’의 개수<br>
dim_V의 값은 정해져있지 않다. 어차피 Linear연산을 마지막에 한번 더 수행하여 z를 얻는다.</p>
<h2 id="attention-score">Attention score</h2>
<p>&ndash;&gt;<img src="https://user-images.githubusercontent.com/75467530/230661507-e2eb7523-ea04-4b1b-bb6c-d8184d4337ca.png" alt="image"></p>
<p>코사인 유사도 공식은 A,B가 유사할 수록 1, 다를 수록 -1에 가까워진다. 이때 A*B를 L2norm의 곱으로 스케일링 해서 값을 구한다.<br>
attention score의 차원은 결국, batch_Q x batch_n 의 사이즈. 각각 대응 하는 단어의 갯수의 곱만큼 score행렬을 만들어야한다.</p>
<p><img src="https://user-images.githubusercontent.com/75467530/230661516-59d5e2ca-1bd5-4f8c-8b6b-f36f30c9da82.png" alt="image"></p>
<p>￼</p>
<h2 id="scaling--softmax">Scaling &amp; Softmax</h2>
<p>Q*K dot product의 특성상, 문장길이 dk가 커질수록 더 큰 값을 가지게 되고, softmax에 넣게 되면, 큰값만 살아남는 현상이 발생, 따라서 값이 작으면 gradient vanishing으로 이어질 수 있다. 따라서 스케일링으로 값을 유사하게 맞춰주는 효과 -&gt; 여기서 dk는 n_K인가?? -&gt; 왜 dk가 커질수록 더 큰값을 가지나??<br>
마지막으로 softmax를 거친 값에 Value를 dot product하면 self-attention값이 구해진다.</p>
<img width="294" alt="스크린샷 2023-03-29 10 22 08" src="https://user-images.githubusercontent.com/75467530/230661524-34e8ab09-1670-4e73-a4bd-2ada958a48fe.png">
<p>￼
Q,K이 각각 i.i.d N(0,1)라고 가정하면, 위와같은 분포를 보인다. 따라서 sqrt(dk)로 나눠서 N(0,1)로 다시 조정하는 역할을 한다.</p>
<h4 id="multihead">Multihead</h4>
<p>multihead를 하는 이유 - 각각의 attention는 토큰 간의 관계(유사도)를 통해 어텐션을 구하고 종속성을 계산하며, 각각의 head는 다른 유형의 종속성을 가지게 된다.(문장타입에집중, 관계에 집중, 명사에집중, 강조에 집중)</p>
<h2 id="reference">Reference</h2>
<ul>
<li><a href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></li>
<li><a href="https://www.blossominkyung.com/deeplearning/transformer-mha">https://www.blossominkyung.com/deeplearning/transformer-mha</a></li>
<li><a href="https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer1-Scaled-Dot-Product-Attention">https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer1-Scaled-Dot-Product-Attention</a></li>
</ul>
</section>

  
  
  <footer class="mt-12 flex flex-wrap">
     
    <a
      class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]"
      href="http://localhost:1313/tags/transformers"
      >Transformers</a
    >
     
    <a
      class="mb-1.5 ltr:mr-1.5 rtl:ml-1.5 rounded-lg bg-black/[3%] px-5 py-1 no-underline hover:bg-black/[6%] dark:bg-white/[8%] dark:hover:bg-white/[12%]"
      href="http://localhost:1313/tags/attention"
      >Attention</a
    >
    
  </footer>
  

  
  
  
  
  <nav
    class="mt-24 flex overflow-hidden rounded-xl bg-black/[3%] text-lg !leading-[1.2] *:flex *:w-1/2 *:items-center *:p-5 *:font-medium *:no-underline dark:bg-white/[8%] [&>*:hover]:bg-black/[2%] dark:[&>*:hover]:bg-white/[3%]"
  >
    
    <a class="ltr:pr-3 rtl:pl-3" href="http://localhost:1313/posts/paper/attention-is-all-you-need-3/"
      ><span class="ltr:mr-1.5 rtl:ml-1.5">←</span><span>[논문리뷰] Attention is all you need #3</span></a
    >
    
    
    <a class="ltr:ml-auto rtl:mr-auto justify-end pl-3" href="http://localhost:1313/posts/paper/attention-is-all-you-need-1/"
      ><span>[논문리뷰] Attention is all you need #1</span><span class="ltr:ml-1.5 rtl:mr-1.5">→</span></a
    >
    
  </nav>
  
  

  
  

  
  

  


  
</article>


    </main>

    <footer
  class="mx-auto flex h-[4.5rem] max-w-[--w] items-center px-8 text-xs uppercase tracking-wider opacity-60"
>
  <div class="mr-auto">
  
    &copy; 2024
    <a class="link" href="http://localhost:1313/">누누타운</a>
  
  </div>
  <a class="link mx-6" href="https://gohugo.io/" rel="noopener" target="_blank"
    >powered by hugo️️</a
  >️
  <a
    class="link"
    href="https://github.com/nanxiaobei/hugo-paper"
    rel="noopener"
    target="_blank"
    >hugo-paper</a
  >
</footer>

  </body>
</html>
