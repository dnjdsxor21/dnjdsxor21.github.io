<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>누누타운</title>
    <link>https://dnjdsxor21.github.io/</link>
    <description>Recent content on 누누타운</description>
    <generator>Hugo</generator>
    <language>ko-KR</language>
    <lastBuildDate>Thu, 12 Dec 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://dnjdsxor21.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Blog] Markdown Syntax 마크다운 가이드</title>
      <link>https://dnjdsxor21.github.io/posts/blog/markdown-guide/</link>
      <pubDate>Thu, 12 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/blog/markdown-guide/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;Markdown is a lightweight markup language that you can use to add formatting elements to plaintext documents.&lt;/p&gt;&#xA;&lt;/blockquote&gt;&#xA;&lt;p&gt;Markdown은 문서를 작성하는 하나의 언어입니다.&lt;br&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;code&gt;github&lt;/code&gt;,&lt;code&gt;notion&lt;/code&gt;,&lt;code&gt;tistory&lt;/code&gt;, &lt;code&gt;velog&lt;/code&gt; 등등 다양한 서비스에서 지원하기 때문에 편리하게 작성이 가능합니다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;level-1-pushpin&#34;&gt;LEVEL 1 &amp;#x1f4cc;&lt;/h2&gt;&#xA;&lt;h2 id=&#34;헤더-headers&#34;&gt;헤더 (Headers)&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-markdown&#34; data-lang=&#34;markdown&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;# h1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## h2&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;### h3&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#### h4&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;##### h5&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;###### h6&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;h1&#34;&gt;h1&lt;/h1&gt;&#xA;&lt;h2 id=&#34;h2&#34;&gt;h2&lt;/h2&gt;&#xA;&lt;h3 id=&#34;h3&#34;&gt;h3&lt;/h3&gt;&#xA;&lt;h4 id=&#34;h4&#34;&gt;h4&lt;/h4&gt;&#xA;&lt;h5 id=&#34;h5&#34;&gt;h5&lt;/h5&gt;&#xA;&lt;h6 id=&#34;h6&#34;&gt;h6&lt;/h6&gt;&#xA;&lt;p&gt;헤더는 &amp;lsquo;#&amp;rsquo; 기호를 사용하여 표시합니다. &amp;lsquo;#&amp;lsquo;의 개수에 따라 h1부터 h6까지 다양한 크기의 헤더를 만들 수 있습니다. &lt;strong&gt;h1은 가장 큰 헤더이며, h6로 갈수록 크기가 작아집니다.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>[공공데이터] 공공데이터포털 / OPENAPI #1</title>
      <link>https://dnjdsxor21.github.io/posts/openapi/openapi-1/</link>
      <pubDate>Tue, 05 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/openapi/openapi-1/</guid>
      <description>&lt;h2 id=&#34;heading&#34;&gt;&lt;/h2&gt;&#xA;&lt;p&gt;여러 산업에서 데이터가 최우선의 가치를 가지고 있습니다.&lt;br&gt;&#xA;따라서 데이터를 잘 축적하고 가공하여 개방하는 것도 중요한 임무가 되었습니다.&#xA;&lt;br&gt;&lt;br&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;좋은-품질의-데이터를-얻을-수-있는-사이트&#34;&gt;좋은 품질의 데이터를 얻을 수 있는 사이트&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;공공데이터포털(&lt;a href=&#34;https://data.go.kr&#34;&gt;https://data.go.kr&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;AI Hub(&lt;a href=&#34;https://aihub.or.kr&#34;&gt;https://aihub.or.kr&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;li&gt;Kaggle(&lt;a href=&#34;https://www.kaggle.com&#34;&gt;https://www.kaggle.com&lt;/a&gt;)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;대한민국은 공공데이터포털을 통해 아주 다양한 분야의 공공데이터를 개방하고 있습니다.&lt;br&gt;&#xA;뿐만 아니라 손쉬운 활용을 위해 API를 제공하고,&lt;br&gt;&#xA;정부 각 부처에서 공공데이터 활용 경진대회를 열고 있습니다.&lt;br&gt;&lt;/p&gt;&#xA;&lt;img src=&#34;https://dnjdsxor21.github.io/img/posts/openapi-1-2.jpg&#34; alt=&#34;data.go.kr&#34; style=&#34;max-width: 80%; height: auto; display: block;&#34; /&gt;&#xA;&lt;p&gt;대한민국은 2015년부터 OECD 공공데이터 개방지수에서&lt;br&gt;&#xA;4년 연속 세계 1위를 기록하는 중입니다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Docker]docker container 도커 #3</title>
      <link>https://dnjdsxor21.github.io/posts/docker/docker-container-guide/</link>
      <pubDate>Tue, 18 Jul 2023 15:52:10 +0900</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/docker/docker-container-guide/</guid>
      <description>&lt;h2 id=&#34;docker-container&#34;&gt;Docker Container&lt;/h2&gt;&#xA;&lt;p&gt;도커 이미지를 만들었다면, 이미지 기반의 컨테이너를 생성할 차례&lt;/p&gt;&#xA;&lt;p&gt;먼저 docker image를 local로 가져오기(나는 local에 있기 때문에 생략 가능)&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker pull dnjdsxor21/fastapi:v1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이미지 확인! &lt;code&gt;docker images&lt;/code&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;container-run&#34;&gt;Container run&lt;/h2&gt;&#xA;&lt;p&gt;본격적으로 컨테이너 띄워보기&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run &amp;lt;image_name&amp;gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이때 줄 수 있는 옵션이 굉장히 많다.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker run -p 3000:80 --name my-container -d &amp;lt;image_name&amp;gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;-p&lt;/code&gt; : port옵션, &lt;code&gt;Dockerfile&lt;/code&gt;에서 작성했던 port80을 실제로 열어주는 작업. 내 컴퓨터(또는 AWS EC2, GCP VM instance)의 포트 3000과 컨테이너의 포트 80을 연결&lt;br&gt;&#xA;&lt;code&gt;--name&lt;/code&gt; : 컨테이너의 이름을 지정. 안하면 랜덤으로 이름 지정&lt;br&gt;&#xA;&lt;code&gt;d&lt;/code&gt;: detach모드를 의미, &lt;code&gt;docker run&lt;/code&gt;는 디폴트로 attach모드를 가진다. attach모드는 컨테이너의 터미널과 연결 되는 것을 의미한다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Docker]docker image 도커 #2</title>
      <link>https://dnjdsxor21.github.io/posts/docker/docker-image-guide/</link>
      <pubDate>Sun, 16 Jul 2023 15:52:10 +0900</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/docker/docker-image-guide/</guid>
      <description>&lt;h2 id=&#34;도커-설치&#34;&gt;도커 설치&lt;/h2&gt;&#xA;&lt;p&gt;&lt;code&gt;Windows&lt;/code&gt;, &lt;code&gt;MacOS&lt;/code&gt;의 경우 &lt;strong&gt;Docker Desktop&lt;/strong&gt;을 설치&lt;br&gt;&#xA;&lt;code&gt;Linux&lt;/code&gt;의 경우 &lt;strong&gt;Docker Engine&lt;/strong&gt;을 설치한다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;이미지-image&#34;&gt;이미지 Image&lt;/h2&gt;&#xA;&lt;p&gt;도커는 이미지와 컨테이너의 개념만 알면 일단 사용이 가능하다. 컨테이너의 기반이 되는 이미지가 필요한데, dockerhub에서 가져오거나 직접 만들 수 있다.&lt;br&gt;&#xA;이미지의 이름은 &lt;code&gt;name:tag&lt;/code&gt;의 방식을 가진다.(&lt;code&gt;python:3.9&lt;/code&gt;, &lt;code&gt;node:14&lt;/code&gt;)&lt;br&gt;&#xA;tag는 보통 버전을 명시&lt;/p&gt;&#xA;&lt;h4 id=&#34;dockerhub에서-가져오기&#34;&gt;dockerhub에서 가져오기&lt;/h4&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;$ docker pull python:3.9&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;dockerfile-작성&#34;&gt;Dockerfile 작성&lt;/h4&gt;&#xA;&lt;p&gt;보통 자신의 개발환경에 맞게 세팅해야 하므로, 기존의 pre-built Image를 가져와서 커스터마이징 한다.&lt;br&gt;&#xA;이를 위해서는 자신의 코드, 패키지 정보 등을 준비하고, &lt;code&gt;Dockerfile&lt;/code&gt;이라는 이름의 파일을 생성한다(이름은 고정)&lt;/p&gt;</description>
    </item>
    <item>
      <title>[Docker]What is Docker 도커 #1</title>
      <link>https://dnjdsxor21.github.io/posts/docker/what-is-docker/</link>
      <pubDate>Sat, 15 Jul 2023 15:52:10 +0900</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/docker/what-is-docker/</guid>
      <description>&lt;h2 id=&#34;what-is-docker&#34;&gt;What is Docker?&lt;/h2&gt;&#xA;&lt;p&gt;A tool for creating and managing &lt;strong&gt;containers&lt;/strong&gt;&lt;br&gt;&#xA;컨테이너를 관리하는 툴&lt;/p&gt;&#xA;&lt;h2 id=&#34;컨테이너는-무엇인가&#34;&gt;컨테이너는 무엇인가?&lt;/h2&gt;&#xA;&lt;p&gt;A Standardized unit of software. A Package of code and dependencies to run that code&lt;br&gt;&#xA;컨테이너는 라이브러리, 코드, 시스템 도구 등을 포함하는 독립적인 환경으로, 마치 하나의 VM(virtual machine)처럼 작동한다고 생각하면 이해하기 쉽다.&lt;/p&gt;&#xA;&lt;p&gt;같은 코드를 공유하다보면 패키지의 버전이 맞지 않거나, OS가 달라서 작동 결과가 다른 경우가 있다. 이때 같은 OS, 같은 패키지 버전에서 어플리케이션을 실행 할 수 있다!&lt;br&gt;&#xA;컨테이너는 **&amp;ldquo;이미지&amp;rdquo;**라는 일종의 템플릿으로부터 실행 할 수 있다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Huggingface 모델 저장하고 불러오기</title>
      <link>https://dnjdsxor21.github.io/posts/nlp/how-to-save-and-load-huggingface/</link>
      <pubDate>Tue, 09 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/nlp/how-to-save-and-load-huggingface/</guid>
      <description>&lt;h2 id=&#34;huggingface-를-사용하면-모델-불러오기-저장이-훨씬-쉽다&#34;&gt;HuggingFace 를 사용하면 모델 불러오기, 저장이 훨씬 쉽다.&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AutoModel&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MODEL_NAME &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;klue/bert-base&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 학습 전 모델 불러오기&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoModel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(MODEL_NAME)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 학습 후 모델 저장하기&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;save_pretrained(OUTPUT_DIR)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;huggingface-hub&#34;&gt;HuggingFace Hub&lt;/h2&gt;&#xA;&lt;p&gt;허깅페이스에도 깃허브처럼 &amp;lsquo;모델&amp;rsquo;과 &amp;lsquo;데이터셋&amp;rsquo;을 저장할 수 있는 저장공간이 존재한다!&lt;/p&gt;&#xA;&lt;p&gt;위에서는 &lt;code&gt;klue/bert-base&lt;/code&gt;를 허브에서 불러왔으며, OUTPUT_DIR(local 환경)에 저장했다.&lt;/p&gt;&#xA;&lt;p&gt;먼저 local 환경(내 컴퓨터)에서 모델을 불러와보자.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AutoModel&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;MODEL_PATH &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;/opt/ml/bert/checkpoint-100&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoModel&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(MODEL_PATH)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;사실 위와 똑같은 방법이다.&lt;br&gt;&#xA;이때 &lt;code&gt;MODEL_PATH&lt;/code&gt;에는 &lt;code&gt;config.json&lt;/code&gt;, &lt;code&gt;pytorch_model.bin&lt;/code&gt; 등등의 파일이 있어야함!&lt;br&gt;&#xA;코드가 같은 이유는 허깅페이스 허브에 모델이 있는지 확인하고, 없으면 로컬 디렉토리에 모델이 있는지 체크한다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Huggingface BERT 모델(BertModel, BertForSequenceClassification, BertForMaskedLM, BertForTokenClassification)</title>
      <link>https://dnjdsxor21.github.io/posts/nlp/huggingface-bert-model/</link>
      <pubDate>Sun, 07 May 2023 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/nlp/huggingface-bert-model/</guid>
      <description>&lt;h2 id=&#34;bert-모델-사용하기&#34;&gt;BERT 모델 사용하기&lt;/h2&gt;&#xA;&lt;p&gt;허깅페이스에서 BERT 모델을 불러와서 사용할 때 목적에 맞게 다양한 함수를 불러온다.&lt;/p&gt;&#xA;&lt;p&gt;예를 들어, STS(Semantic Textual Similarity) task에서 두 문장이 비슷한지 아닌지를 분류하는 문제를 풀기 위해, CLS토큰을 사용하여 학습을 할 것이다. &lt;code&gt;BertForSequenceClassification&lt;/code&gt;를 사용하면 &amp;ldquo;BERT모델의 CLS토큰에 분류를 위한 Linear모델을 추가한 모델&amp;quot;을 불러올 수 있다.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; transformers &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; BertForSequenceClassification&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BertForSequenceClassification(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;klue/bert-base&amp;#39;&lt;/span&gt;, num_labels&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;또는 BERT의 pre-training방법중 하나인 MLM을 수행하기 위해 &lt;code&gt;BertForMaskedLM&lt;/code&gt;을 사용하면 쉽게 모델을 불러올 수 있다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;bertmodel&#34;&gt;BertModel&lt;/h2&gt;&#xA;&lt;p&gt;BERT 기본 모델을 불러오려면 역시 &lt;code&gt;BertModel&lt;/code&gt;를 사용 해야 할 것이다.(또는 &lt;code&gt;AutoModel&lt;/code&gt;)&lt;/p&gt;</description>
    </item>
    <item>
      <title>[논문리뷰] Attention is all you need #3</title>
      <link>https://dnjdsxor21.github.io/posts/paper/attention-is-all-you-need-3/</link>
      <pubDate>Tue, 04 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/paper/attention-is-all-you-need-3/</guid>
      <description>&lt;h2 id=&#34;multihead-attention&#34;&gt;Multihead attention&lt;/h2&gt;&#xA;&lt;p&gt;multihead를 하는 이유 - 각각의 attention는 토큰 간의 관계(유사도)를 통해 어텐션을 구하고 종속성을 계산하며, 각각의 head는 다른 유형의 종속성을 가지게 된다.&lt;/p&gt;&#xA;&lt;p&gt;(문장타입에집중, 관계에 집중, 명사에집중, 강조에 집중)&lt;/p&gt;&#xA;&lt;h2 id=&#34;논문과-실제-코드에서-나타나는-차이점&#34;&gt;논문과 실제 코드에서 나타나는 차이점&lt;/h2&gt;&#xA;&lt;p&gt;논문 : Q,K,V를 각각의 dimension size로 계산해서 똑같이 z를 얻는다 -&amp;gt; z를 모두 concat -&amp;gt; linear연산으로 차원을 축소시켜서 input dimension과 동일하게 맞춰준다.&lt;br&gt;&#xA;따라서 concate(z)의 size = n_head x n_Q x dim_V 이고, linear를 지났을 때의 output의 size = n_Q x dim_input&lt;/p&gt;</description>
    </item>
    <item>
      <title>[논문리뷰] Attention is all you need #2</title>
      <link>https://dnjdsxor21.github.io/posts/paper/attention-is-all-you-need-2/</link>
      <pubDate>Mon, 03 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/paper/attention-is-all-you-need-2/</guid>
      <description>&lt;h2 id=&#34;attention-self-attention&#34;&gt;Attention, Self-attention&lt;/h2&gt;&#xA;&lt;p&gt;일반적으로 어텐션이란, 입력값에서 중요한 단어들에 더 집중할 수 있도록 한다.&lt;br&gt;&#xA;문장을 병렬적으로 처리하기 때문에 속도가 비교적 빠르다.&lt;br&gt;&#xA;RNN구조에서는 순차적으로 값을 확인하기 때문에 gradient vanishing/exploding문제가 발생하게 된다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;self-attention&#34;&gt;Self-attention&lt;/h2&gt;&#xA;&lt;p&gt;같은 문장 내에서 단어들 간의 관계를 나타낸다. attention의 입력값은 Q,K,V&lt;/p&gt;&#xA;&lt;h2 id=&#34;qkv&#34;&gt;Q,K,V&lt;/h2&gt;&#xA;&lt;p&gt;어텐션의 목표는 value의 weighted sum을 구하는것이고, 각 가중치는 Q,K가 얼마나 유사한지에 따라 결정된다.&lt;br&gt;&#xA;Query - 소스벡터, 유사도를 계산하는 값&lt;br&gt;&#xA;Key - 타겟벡터, 유사도를 계산하는 값&lt;br&gt;&#xA;Value - key에 해당하는 정보로 값을 계산, 최종 출력계산에 사용&lt;br&gt;&#xA;각각의 값은 인풋이 Linear연산을 거쳐서 구한다. - 벡터의 차원을 줄여준다.&lt;br&gt;&#xA;Q.size = n x d_K&lt;br&gt;&#xA;K.size = m x d_K&lt;br&gt;&#xA;V. size = m x d_V&lt;br&gt;&#xA;Z.size = n x d_V&lt;br&gt;&#xA;왜 실습에서는 3차원이지? -&amp;gt; 각 단어의 갯수만큼 연산하므로 n_Q, n_K, n_V가 존재&lt;br&gt;&#xA;Q, K 의 dimension은 동일해야함&lt;br&gt;&#xA;n_batch가 의미하는 것????? - data의 개수, ‘문장’의 개수&lt;br&gt;&#xA;dim_V의 값은 정해져있지 않다. 어차피 Linear연산을 마지막에 한번 더 수행하여 z를 얻는다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[논문리뷰] Attention is all you need #1</title>
      <link>https://dnjdsxor21.github.io/posts/paper/attention-is-all-you-need-1/</link>
      <pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/paper/attention-is-all-you-need-1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://user-images.githubusercontent.com/75467530/230660894-458594f7-9c04-45f9-9659-5be94e85398d.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;워드-임베딩&#34;&gt;워드 임베딩&lt;/h2&gt;&#xA;&lt;p&gt;각 단어들은 Word2Index를 통해 정수인코딩을 하고, 그 정수값을 임베딩 벡터로 변환한다.&lt;br&gt;&#xA;이때 논문에서 임베딩dim 은 512이다.&lt;/p&gt;&#xA;&lt;h2 id=&#34;pepositional-encoding&#34;&gt;PE(positional encoding)&lt;/h2&gt;&#xA;&lt;p&gt;트랜스포머 이전에는 RNN모델을 많이 사용했다. 순차적으로 문장을 처리하기 때문에, 계산유닛이 많아도 앞에서부터 하나씩 처리한다. 결국 연산속도가 매우 느리다.&lt;br&gt;&#xA;트랜스포머는 문장을 병렬적으로, 한번에 처리한다. 병렬적으로 한번에 처리한다는 것은, 단어의 위치를 알 수 없다는 뜻이다. 따라서 PE를 통해 위치순서를 나타낼 필요가 있다.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;모든 PE값은 시퀀스의 길이나 값에 관계없이 동일한 식별자를 가져야한다.&lt;/li&gt;&#xA;&lt;li&gt;모든 PE값은 너무 크면 안된다. 워드 임베딩 값이 상대적으로 작아지게된다.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;PE를 위한 sin, cos함수 - [-1, 1]사이를 주기적으로 반복하기 때문에 긴 문장에서도 위치 벡터값이 너무 커지지 않으면서, 너무 미미한 차이를 보이지도 않는다.&lt;br&gt;&#xA;PE는 단어벡터와 같은 차원의 벡터값이다. 따라서 벡터차원만큼의 주기함수 차원을 가지게 되고, 모든 주기함수의 주기 공배수가 되지 않는 이상 값이 겹치지 않는다.&lt;/p&gt;</description>
    </item>
    <item>
      <title>[RNN, LSTM, GRU] RNN 기반 모델 구조</title>
      <link>https://dnjdsxor21.github.io/posts/nlp/rnn-based-models/</link>
      <pubDate>Fri, 03 Mar 2023 15:52:10 +0900</pubDate>
      <guid>https://dnjdsxor21.github.io/posts/nlp/rnn-based-models/</guid>
      <description>&lt;h1 id=&#34;rnn-recurrent-nueral-network&#34;&gt;RNN (Recurrent Nueral Network)&lt;/h1&gt;&#xA;&lt;p&gt;은닉층에서 나온 결과값이 다시 은닉층으로 돌아가 새로운 입력값과 연산을 하는 구조&lt;/p&gt;&#xA;&lt;p&gt;시계열 데이터, 텍스트 데이터같은 가변길이의 데이터에 적합한 모델&lt;/p&gt;&#xA;&lt;p&gt;학습을 통해 각 단어 다음에 나올 단어를 예측&lt;/p&gt;&#xA;&lt;h3 id=&#34;lstm&#34;&gt;LSTM&lt;/h3&gt;&#xA;&lt;p&gt;hiddne state는 벡터값이고,&#xA;$y = W*h$&lt;/p&gt;&#xA;&lt;p&gt;LSTM의 경우 과거 정보를 가져갈지 말지를 정하는 cell state가 존재.&#xA;여기서 c, h는 같은 크기의 벡터이다. 따라서 가중치행렬 사이즈는 두가지 종류임.&lt;/p&gt;&#xA;&lt;p&gt;$x \rightarrow h$&#xA;$h \rightarrow h$&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;각각의 연산에서 행렬의 곱셈부분이 inner product인지 element-wise product인지 확인할 필요가 있다.&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>https://dnjdsxor21.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/about/</guid>
      <description>&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://github.com/dnjdsxor21&#34;&gt;github.com/dnjdsxor21&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;Email: &lt;a href=&#34;mailto:dnjdsxor21@gmail.com&#34;&gt;dnjdsxor21@gmail.com&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>archives</title>
      <link>https://dnjdsxor21.github.io/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/archives/</guid>
      <description></description>
    </item>
    <item>
      <title>Categories</title>
      <link>https://dnjdsxor21.github.io/categories/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/categories/</guid>
      <description></description>
    </item>
    <item>
      <title>Series</title>
      <link>https://dnjdsxor21.github.io/series/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://dnjdsxor21.github.io/series/</guid>
      <description></description>
    </item>
  </channel>
</rss>
