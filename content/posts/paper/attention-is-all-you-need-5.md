+++
date = '2023-04-06'
draft = true
title =  "[논문리뷰] Attention is all you need #5"
categories = ["Paper"]
tags = ["Transformers", "Attention"]
series = ["Transformer"]
+++

## Masked Multihead Self-attention

<img width="793" alt="스크린샷 2023-03-29 10 39 24" src="https://user-images.githubusercontent.com/75467530/230662201-337365b6-f416-43ba-9216-28d1ddd5fd15.png">

## Cross attention
**Encoder-decoder attention**

![transformer_resideual_layer_norm_3](https://user-images.githubusercontent.com/75467530/230662934-714e2ebc-ce15-4d72-a70b-7de616f5a9ca.png)


## Reference
- <https://tigris-data-science.tistory.com/entry/%EC%B0%A8%EA%B7%BC%EC%B0%A8%EA%B7%BC-%EC%9D%B4%ED%95%B4%ED%95%98%EB%8A%94-Transformer4-Masked-Multi-Head-Attention%EA%B3%BC-Decoder>